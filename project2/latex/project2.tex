

\documentclass[english,notitlepage,nofootinbib]{revtex4-1}  % defines the basic parameters of the document
%For preview: skriv i terminal: latexmk -pdf -pvc filnavn



% if you want a single-column, remove reprint

% allows special characters (including æøå)
\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}

%% note that you may need to download some of these packages manually, it depends on your setup.
%% I recommend downloading TeXMaker, because it includes a large library of the most common packages.

\usepackage{physics,amssymb}  % mathematical symbols (physics imports amsmath)
\include{amsmath}
\usepackage{graphicx}         % include graphics such as plots
\usepackage{xcolor}           % set colors
\usepackage{hyperref}         % automagic cross-referencing (this is GODLIKE)
\usepackage{listings}         % display code
\usepackage{subfigure}        % imports a lot of cool and useful figure commands
\usepackage{float}
%\usepackage[section]{placeins}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{quantikz}
% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}}

%C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
%   NEW COMMANDS
\renewcommand{\vec}{\mathbf}
\newcommand{\transpose}{^\intercal} % transpose
\newcommand{\hx}{\hat{x}}
\newcommand{\closed}[1]{\left( #1 \right)}
\newcommand{\bracket}[1]{\left[ #1 \right]}


\graphicspath{{../output/plots/}} % path for figures

\begin{document}

\title{Project 2 FYS4150}      % self-explanatory
\author{Vetle Vikenes, Johan Mylius Kroken \& Nanna Bryne}          % self-explanatory
\date{\today}                             % self-explanatory
\noaffiliation                            % ignore this, but keep it.


\maketitle 
    
The code is available on GitHub at \url{https://github.com/Vikenes/FYS4150/tree/main/project2}.
    

\section*{Introduction}

To describe a one-dimensional buckling beam, we have the second order differential equation 

\begin{align}\label{eq:unscaled_eq}
    \gamma \dv[2]{u}{x} = -F u(x), \quad x\in [0,L]
\end{align}
where $L$ is the length of the horizontal beam, $F$ is the applied force at the end point $x=L$. $u(x)$ is the vertical displacement of the beam at the horizontal position $x$ and $\gamma$ is a constant determined by the properties of the material. We consider the endpoints to be pinned down, so that $u(0)=u(L)=0$. The endpoints are allowed to rotate, so $u'(x)\neq0$.

\section*{Problem 1}

We want to recast equation \eqref{eq:unscaled_eq} into a dimensionless equation, and we begin by defining the dimensionless length $\hat{x}\equiv x/L$, so that $\hat{x}\in[0,\,1]$. The second derivative with respect to $x$ is then 
\begin{align*}
    \dv[2]{x} = \dv[2]{\hat{x}}{x}\dv[2]{\hat{x}}=\frac{1}{L^2} \dv[2]{\hat{x}}
\end{align*}  

Equation \eqref{eq:unscaled_eq} can therefore be written as 
\begin{align*}
    \gamma \dv[2]{u(x)}{x} &= \frac{\gamma}{L^2}\dv[2]{u(x)}{\hat{x}} = -F u(x) \\ 
    \dv[2]{u(x)}{\hat{x}} &= - \frac{FL^2}{\gamma} u(x) \equiv -\lambda u(x)
\end{align*}
where we defined $\lambda\equiv F L ^2 / \gamma$. Inserting for $x=L\hat{x}$ yields 
\begin{align*}
    L \dv[2]{u(\hat{x})}{\hat{x}} &= -L \lambda u(\hat{x}) 
\end{align*}
Dividing by $L$ on both sides, we arrive at the scaled version of equation equation \eqref{eq:unscaled_eq}

\begin{align}\label{p1_scaled_eq}
    \dv[2]{u(\hat{x})}{\hat{x}} = -\lambda u(\hat{x}).
\end{align}

% COMMENT: As mentioned in the project description, we should mention that u_x(x)=u_\hat{x}(\hat{x})

\section*{Problem 2}
Before implementing the Jacobi algorithm, we create a test that checks whether we set up our tridiagonal matrix $A$ of size $N\cross N$ correctly. $A$ is defined by the signature $(a,\,d,\,a)$, where $a=-1/h^2$ and $d=2/h^2$ with $h$ being the step size $h \equiv(\hx_\mathrm{max}-\hx_\mathrm{min})/n$, where $n=N-1$. 

To test we write a short program in \CC\, that first sets up the tridiagonal $A$ for $N=6$, then uses the Armadillo library to solve the equation
\begin{align}
    A\vec{v} = \lambda \vec{v} \label{eq:eig_equation_general}
\end{align} 
where $\vec{v}$ and $\lambda$ are the eigenvectors and eigenvalues of $A$, respectively. Finally, we compare the eigenvectors and eigenvalues from Armadillo with the analytical result for $N=6$. The analytical solution to equation \eqref{eq:eig_equation_general} for a matrix of size $N\cross N$ is given by  
\begin{align}  
    \lambda^{(i)} &= d + 2a \cos\closed{\frac{i\pi}{N+1}},\quad i=1,\,\dots,\,N \label{eq:eigval}\\
    \vec{v}^{(i)} &= \bracket{ \sin\closed{\frac{i\pi}{N+1}},\,\dots,\,\sin\closed{\frac{Ni\pi}{N+1}} }^T ,\quad i=1,\,\dots,\,N \label{eq:eigvec}
\end{align} 
The resulting eigenvectors of the two methods will be scaled before comparison. 
% COMMENT: Write about test? Comment on our cheeky method of absing the vectors?   

\section*{Problem 3}
The Jacobi algorithm relies on identifying the off-diagonal elements of a matrix with the largest absolute value, which is what we will consider now. 
\subsection*{a)}

In \CC, we write a function that finds the off-diagonal element with the largest absolute value of an $N\cross N$ matrix $A$. It takes the reference to an Armadillo matrix as input, as well as references to two integers, $k$ and $l$. Algorithm \ref{algo:p3_max_off_diag} describes how the finds the desired off-diagonal element, and its corresponding indices, $k$ and $l$. The algorithm specializes to a symmetric matrix, so we consider the upper-right off-diagonal triangle of the matrix only.  

\begin{algorithm}[H]
\caption{Find max off-diagonal element}\label{algo:p3_max_off_diag}
    \begin{algorithmic}
        \Require $A$ symmetric
        \State $k=0$
        \State $l=1$
        \State $max = |A_{kl}|$ 
    
        \For{$i\neq j$ in upper-right triangle of $A$}
            \If{$|A_{ij}| > max$}
                \State $max = |A_{ij}|$
                \State $k=i$
                \State $l=j$
            \EndIf
        \EndFor
        \Return $max$
    \end{algorithmic}
\end{algorithm}

\subsection*{b)}

We test algoritm \ref{algo:p3_max_off_diag} using the matrix 
\begin{align*}
    A=\begin{pmatrix}
        1 & 0 & 0 & 0.5 \\
        0 & 1 & -0.7 & 0 \\
        0 & -0.7 & 1 & 0 \\
        0.5 & 0 & 0 & 1
    \end{pmatrix}.
\end{align*}

For the test to pass, the program should return $0.7$ as maximum value. The corresponding indices should then be row $k=1$ and column $l=2$, since we consider the upper-right triangle. Due to fine coding, the test passed.

\section*{Problem 4}

\subsection*{a)}
We write a code that solves equation \eqref{eq:eig_equation_general} using Jacobi's rotation algorithm. We begin by finding the largest off-diagonal element of $A$ using algorithm \ref{algo:p3_max_off_diag}. Using that result, we apply Jacobi's rotation algorithm, presented in Algorithm \ref{algo:p4_jacobi_rotation}. This process is repeated until Algorithm \ref{algo:p3_max_off_diag} returns an off-diagonal element $max < \epsilon$, where $\epsilon=10^{-8}$ is the chosen tolerance. 
\begin{algorithm}[H]
    \caption{Jacobi rotation}\label{algo:p4_jacobi_rotation}
    \begin{algorithmic}
        \If{$A_{kl}^m = 0$}
            \State $c=1$
            \State $s=0$
            \State $t=0$
        \Else 
            \State $\tau = (A_{ll}^m-A_{kk}^m) / (2A_{kl}^m)$
            \If{$\tau > 0$}
                \State $t= 1/(\tau + \sqrt{1+\tau^2})$
            \Else
                \State $t = -1/(-\tau + \sqrt{1+\tau^2})$
            \EndIf
            \State $c=1/(\sqrt{1+t^2})$
            \State $s=ct$
        \EndIf
        \State $A_{kk}^{m+1} = c^2A_{kk}^m - 2csA_{kl}^m + s^2A_{ll}^m$
        \State $A_{ll}^{m+1} = c^2A_{ll}^m + 2csA_{kl}^m + s^2A_{kk}^m$
        \State $A_{kl}^{m+1} = 0$
        \State $A_{lk}^{m+1} = 0$
        \For{$i=0,1,2,\dots,N-1$}
            \If{$i\neq k \wedge i \neq l$}
                \State $A_{ik}^{m+1} = cA_{ik}^m -sA_{il} $
                \State $A_{ki}^{m+1} = A_{ik}^{m+1}$
                \State $A_{il}^{m+1} = cA_{il}^m + sA_{ik}^m$
                \State $A_{li}^{m+1} = A_{il}^{m+1}$
            \EndIf
            \State $R_{ik}^{m+1} = cR_{ik} - sR_{il}$
            \State $R_{il}^{m+1} = cR_{il} + sR_{ik}$
        \EndFor
    \end{algorithmic}
\end{algorithm}
\subsection*{b)}
To test the rotation algorithm, we compare its resulting eigenvalues and eigenvectors with the analytical solutions in equations \eqref{eq:eigval} and \eqref{eq:eigvec}, respectively. $N=6$ is our choice of matrix size for comparison, with $(-1/h^2,\,2/h^2,\,-1/h^2)$ as the signature for the triangular matrix. It worked brilliantly, as expected.


\section*{Problem 5}

Consider our symmetric, tridiagonal matrix $A \in \mathbb{R}^{N\cross N}$ with signature $(a,d,a)$. Let $M$ denote the number of transformations needed for a Jacobi rotation algorithm to converge. That is, $M$ represents the number of iterations in the Jacobi rotation algorithm needed for the transformed matrix $A'$ to be similar enough\footnote{Choosing the number $\varepsilon=10^{-8}$ to be \textit{close enough} to zero.} to a diagonal matrix.

For $N=2,3,\dots,100$, we run the Jacobi eigensolver and save the corresponding integer $M$. We do this for both the tridiagonal matrix $A$ and an arbitrary symmetric square (dense) matrix $A^*$ of the same size as $A$. The result is plotted in Figure \ref{fig:p5_transformations_per_N}. From the graph in the figure we deduce that the number of transformations scales approximately exponential with matrix size $N$ for the tridiagonal matrix $A$. 


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{jacobi_comparison.pdf}
    \caption{The number of transformations $M$ as function of the matrix size $N$.}\label{fig:p5_transformations_per_N}
\end{figure}

Similar behaviour is seen for matrix $A^*$ which is not a tridiagonal matrix. We would perhaps expect a tridiagonal matrix like $A$ to converge with fewer transformations, since most of its entries are already zero. However, by running the Jacobi rotation algorithm on a tridiagonal matrix you might end up replacing zero (or close to zero) values with non-zero value in the attempt of rotating once. This mean you might have to put the same element to zero more than once and thus do more transformation that we initially would think. This is emphasized in figure Figure \ref{fig:p5_transformations_per_N} where the $M$-dependency of $A$ and $A^*$ are approximately the same.   


\section*{Problem 6}

\subsection*{a)}

For $n=10$ steps, i.e. $n+1=11$ points $\hat{x}_i$ and $A\in\mathbb{R}^{(n-1)\cross(n-1)} =\mathbb{R}^{9\cross 9}$, we solve the eigenvalue problem $A\vec{v} = \lambda \vec{v}$ using the Jacobi eigensolver. In addition, we solve the same problem with the analytic expressions for $\lambda^{(i)}$ and $\vec{v}^{(i)}$, found using equations \ref{eq:eigval} and \ref{eq:eigvec} respectively. This yields two versions of the normalised vectors $\vec{v}^{(i)}$, and for some $i$'s these are counter-oriented. When we encounter this situation, the issue is solved by forcing the result from the Jacobi algorithm $\vec{v}^{(i)} \rightarrow - \vec{v}^{(i)}$.

The three resulting eigenvectors $\vec{v}^{(1)}$, $\vec{v}^{(2)}$ and $\vec{v}^{(3)}$ corresponding to eigenvalues $\lambda^{(1)}$, $\lambda^{(2)}$ and $\lambda^{(3)}$ s.t. $\lambda^{(i)}<\lambda^{(j)}$ for $i<j$, are plotted in Figure \ref{fig:p6_solution_10steps}. The vectors are extended with the boundary points, i.e. $v_0^{(i)}=v_0 = 0$ and $v_n^{(i)}=v_n=0$ for all $i$.

% figure or Figure?

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{solution_10steps.pdf}
    \caption{The first three eigenvectors $\vec{v}^{(i)}$ respectively corresponding to the three lowest eigenvalues $\lambda^{(i)}$ computed with the Jacobi eigensolver using $n=10$ discretisation steps. The white overplotted graphs are the predictions from the analytic expression.}\label{fig:p6_solution_10steps}
\end{figure}

\subsection*{b)}
\begin{centering}
We do exactly the same as in \textbf{a)},\\
using $n=100$ today! \\
So now you can see, \\
as presented in \ref{fig:p6_solution100steps},\\
the results in a very good way!\\
\end{centering}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{solution_100steps.pdf}
    \caption{The first three eigenvectors $\vec{v}^{(i)}$ respectively corresponding to the three lowest eigenvalues $\lambda^{(i)}$ computed with the Jacobi eigensolver using $n=100$ discretisation steps. The white overplotted graphs are the predictions from the analytic expression.}\label{fig:p6_solution100steps}
\end{figure}


\section*{Acknowledgments}
We want to express our most sincere gratitudes to our code for passing all the tests we have presented to it. This work could not have completed without the funding of Group teacher salaries, and its subsequent investment in Ringnes\footnote{Org. nr. 989 668 137 - RINGNES NORGE AS}.


\end{document}









