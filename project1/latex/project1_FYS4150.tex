\documentclass[english,notitlepage,nofootinbib]{revtex4-1}  % defines the basic parameters of the document
%For preview: skriv i terminal: latexmk -pdf -pvc filnavn




\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}


\usepackage{physics,amssymb} 
\include{amsmath}
\usepackage{graphicx}        
\usepackage{xcolor} 
\usepackage{hyperref} 
\usepackage{listings}     
\usepackage{subfigure}    
\usepackage{float}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{subfigure}
\usepackage{tikz}
\normalsize
\usepackage{booktabs, siunitx}

% \usepackage[toc,page]{appendix}

%   NEW COMMANDS
\renewcommand{\vec}{\mathbf}

% Commands for consitent refernecing (Nanna's convention)
\newcommand{\Table}[1]{Table \ref{table:#1}}
\newcommand{\Fig}[1]{Figure \ref{fig:#1}}
\newcommand{\Eq}[1]{eq. (\ref{eq:#1})}
\newcommand{\Sec}[1]{section \ref{sec:#1}}
\newcommand{\Algo}[1]{Algorithm \ref{algo:#1}}

%C++
\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}

\usetikzlibrary{quantikz}
% defines the color of hyperref objects
% Blending two colors:  blue!80!black  =  80% blue and 20% black
\hypersetup{ % this is just my personal choice, feel free to change things
colorlinks,
linkcolor={red!50!black},
citecolor={blue!50!black},
urlcolor={blue!80!black}}

\graphicspath{{../output/plots/}} % path for figures
\begin{document}

\title{Project 1 FYS4150} 
\author{Vetle A. Vikenes, Johan Mylius Kroken and Nanna Bryne}      
\date{\today}                 
\noaffiliation       

\maketitle 
The code can be found on GitHub at \url{https://github.com/Vikenes/FYS4150/tree/main/project1}.
    
\section*{Introduction}
We will solve the one-dimensional Poisson equation 
\begin{align} \label{eq:poisson_eq}
    - \dv[2]{u}{x}=f(x)
\end{align}
where the source function, $f(x)=100e^{-10x}$, is known. We will do this for $x\in[0,\,1]$ with boundary conditions $u(0)=u(1)=0$.

\section*{Problem 1}

We want to check that 

\begin{align}\label{eq:p1_analytical_sol}
    u(x) = 1 - (1-e^{-10})x - e^{-10x}
\end{align}

is the solution of eq. \eqref{eq:poisson_eq} given our source function, $f(x)$. We first control that eq. \eqref{eq:p1_analytical_sol} satisfies the boundary conditions. 

\begin{align*}
    u(0) &= 1 - 0 - e^{0} = 0 \\
    u(1) &= 1 - (1-e^{-10}) - e^{-10} = 0
\end{align*}
 
We find the double derivative of $u(x)$,

\begin{align*}
    \dv{u}{x} &= -(1-e^{-10}) - (-10)e^{-10x} \quad \Rightarrow \quad \dv[2]{u}{x} = -100e^{-10x}=-f(x),
\end{align*}

and see that $u(x)$ in eq. \eqref{eq:p1_analytical_sol} satisfies equation \eqref{eq:poisson_eq}, with our given source function.


\section*{Problem 2}

We write a program in \CC\ that defines a vector of linearly spaced values of $x\in[0,1]$, evaluates the exact solution from \Eq{p1_analytical_sol} at these points and saves the information. We then use Python to read the data and plot the solution, which is shown in figure \ref{fig:p1_analytical_sol}.

% Add proper plot. Comment plot 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{ux.pdf}
    \caption{The analytical solution in \Eq{p1_analytical_sol}.}
    \label{fig:p1_analytical_sol}
\end{figure}

\section*{Problem 3}
For a discrete value of $x$ at a point $i$, we denote this as $x_i$, where the corresponding function value is defined as $u(x_i)\equiv u_i$. Similarly, the source function is defined as $f(x_i)\equiv f_i$. We now find the discretized version of the second order derivative of $u_i$
\begin{align*}
    \dv[2]{u}{x}\bigg\rvert_{x_i} = u_i'' = \frac{u_{i-1}-2u_i+u_{i+1}}{h^2} + \mathcal{O}(h^2) \approx \frac{v_{i-1}-2v_i+v_{i+1}}{h^2} = v_i''
\end{align*}
where $v_i$ is the approximated value of $u_i$ obtained by neglecting the $\mathcal{O}(h^2)$ term, and $h=(x_{\mathrm{max}}-x_{\mathrm{min}})/n_{\mathrm{steps}}$ is the step length. % Should define h differently

Inserting for $v_i$ into the Poisson equation yields 
\begin{align}
    -v_i'' = \frac{-v_{i-1}+2v_i-v_{i+1}}{h^2} &= f_i \label{eq:p3_disc_poisson_eq} 
\end{align}
which is a discretized version of the Poisson equation. 



\section*{Problem 4}
% Should be rewritten to be "compatible" with P5 and P6 once those tasks are completed. 
The computations performed in equation \eqref{eq:p3_disc_poisson_eq} to obtain a value for $f_i$ can be expressed in terms of vectors. If we have two column vectors, $\mathbf{a}$ and $\mathbf{\tilde{v}}$, defined as $\mathbf{a}=(-1,\,2,\,-1)$ and $\mathbf{\tilde{v}}=(v_{i-1},\,v_i,\,v_{i+1})$, respectively, equation \eqref{eq:p3_disc_poisson_eq} can be written as  
\begin{align*}
    \mathbf{a}^T \, \mathbf{\tilde{v}} = \begin{pmatrix}
        -1 & 2 & -1 
    \end{pmatrix}
    \begin{pmatrix}
        v_{i-1} \\ 
        v_i \\ 
        v_{i+1}
    \end{pmatrix}
    = -v_{i+1}+2v_i-v_{i-1} = h^2 f_i \equiv g_i.
\end{align*}
We can extend this to all $n$ values of $x_i$, computing $\mathbf{g}=(g_0,\,\dots,\,g_{n-1})$ by multiplying $\mathbf{v}=(v_0,\,\dots,\,v_{n-1})$ with a tridiagonal matrix $A$ with $2$ on its main diagonal and $-1$ on its subdiagonal and superdiagonal. However, for $v_0$ and $v_{n-1}$ we are unable to compute the second derivative, since $v_{-1}$ and $v_{n}$ are not defined. From the boundary conditions we have $v_0=v_{n-1}=0$, so $v_1$, $v_{n-2}$ and all the elements in between can be computed. Omitting the end points, equation \eqref{eq:p3_disc_poisson_eq} can be written as a matrix equation by  
\begin{align} \label{eq:p4_matrix_eq}
    A\mathbf{v} = 
    \begin{pmatrix}
        2 & -1 & & \\
        -1 & \ddots & \ddots & \\ 
         & \ddots & & -1 \\
          & & -1 & 2
    \end{pmatrix}
    \begin{pmatrix}
        v_1 \\
        v_2 \\
        \vdots \\ 
        v_{n-2}
    \end{pmatrix} = 
    \begin{pmatrix}
        g_1 \\
        g_2 \\ 
        \vdots \\
        g_{n-2}
    \end{pmatrix}
    = \mathbf{g},
\end{align}
where the value of the original differential equation is obtained by $\mathbf{f}=\mathbf{g}/h^2$. For the end-points we actually have $2v_1-v_2=g_1+v_0$ and $-v_{n-3}+2v_{n-2}=g_{n-2}+v_{n-1}$, but we have ommited these additional terms as they both are zero. 


\section*{Problem 5}
% Comment: Fix indices in the end 
\subsection*{a)}
We have two vectors of length $m$, $\mathbf{v}^*$ and $\mathbf{x}$, representing a complete solution to the discretized Poisson equation and corresponding $x$ values, respectively. With a matrix $A$ being the tridiagonal matrix from problem 4, we can find how $n$, relates to $m$. For all elements of $\mathbf{v}^*$ to be a complete solution, the derivative in equation \eqref{eq:p3_disc_poisson_eq} must be applicable, hence $\vec{v}^*=(v_1, v_2, ..., v_{n-2})$ lacks the boundaries and is therefore two elements "shorter" than $\vec{v}$, i.e. $m=n-2$. 

\subsection*{b)}
When solving \eqref{eq:p4_matrix_eq} for $\mathbf{v}$, we get all the elements of $\vec{v}^*$. The remaining elements of $\mathbf{v}$ that are missing, are the end points, which is given by the boundary conditions, $v_0=v_{n-1}=0$.
% Comment: Write 5b)


\section*{Problem 6}
\subsection*{a)}
We now concern ourselves with the general solution of the matrix equation $A\vec{v} = \vec{g}$ where $A$ is a \textit{general} $n\cross n$ tridiagonal matrix. We thus have the following:

\begin{align*}
    A\mathbf{v} = 
    \begin{pmatrix}
        b_1 & c_1 & & \\
        a_2 & \ddots & \ddots & \\ 
         & \ddots & & c_{n-1} \\
          & & a_n & b_n
    \end{pmatrix}
    \begin{pmatrix}
        v_1 \\
        v_2 \\
        \vdots \\ 
        v_{n}
    \end{pmatrix} = 
    \begin{pmatrix}
        g_1 \\
        g_2 \\ 
        \vdots \\
        g_{n}
    \end{pmatrix}
    = \mathbf{g}.
\end{align*}
In order to find a  general solution for $\vec{v}$ for such a tridiagonal matrix we use the method of Gaussian elimination. In the end, we end up with an algorithm called the Thomas algorithm\footnote{The complete derivation is given in Appendix \ref{Apx:Thomas_algorithm}}.


We summarize the algorithm as follows:

\begin{algorithm}[H]
    \caption{General algorithm}\label{algo:p6_general_algorithm}
    \begin{algorithmic}
        \State $\tilde{b}_0 = b_0$ \Comment{Define initial values}
        \State $\tilde{g}_0 = g_0$
        \For{$i = 1, 2, ..., m-1$} \Comment{Do forward substitution}
        \State $K = a_i / \tilde{b}_{i-1}$
        \State $\tilde{b}_i=b_i - K \cdot  c_{i-1}$
        \State $\tilde{g}_i=g_i - K \cdot \tilde{g}_{i-1}$
        \EndFor \\
        \State $v_{m-1} = \tilde{g}_{m-1} / \tilde{b}_{m-1}$ \Comment{Define initial value}
        \For{$i = m-2, m-3, ..., 0$} \Comment{Do backward substitution}
        \State $v_i = (\tilde{g}_i - v_{i+1} \cdot  c_i) / \tilde{b}_i$ 
        \EndFor
    \end{algorithmic}
\end{algorithm}


\subsection*{b)}

The first loop in \Algo{p6_general_algorithm} contains $1+2+2=5$ FLOPs and runs $m-2$ times. In the second loop we perform $1+1+1=3$ FLOPs per iteration. Remembering the operation prior to the second loop gives a total of $5(m-2)+1+3(m-2)=8(m-2)+1$ FLOPs.

% Comment: 6b) - Count FLOPs

\section*{Problem 7}

\subsection*{a)}

We implement the general algorithm, \Algo{p6_general_algorithm}, in \CC\, and let $A$ be the matrix from Problem 4, i.e. $a_i=c_i=-1$ and $b_i=2$. For a given number of discretization steps, $n_\text{steps}$, the code saves the solution as pairs $(x_i, v_i)$ to a file.


\subsection*{b)}

We compute $\mathbf{v}$ with the general algorithm for $n_\text{steps}=10,\, 100 \text{ and } 1000$ and plot the numerical solutions up against the analytic solution in \Fig{p7_num_sol}. We see that $n_\text{steps}=10$ produces a large deviation from the exact solution, whereas $n_\text{steps}=100$ yields a far smaller deviation. Choosing $n_\text{steps}=1000$ reproduces a good approximation to the exact solution, with no visible deviation. To quantify the validity of these results, we proceed by studying the errors from the different step sizes.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{comparison_p7.pdf}
    \caption{The numerical solution to the Poisson equation using \Algo{p6_general_algorithm} with different choices of $n_\text{steps}$, compared the the exact solution.}\label{fig:p7_num_sol}
\end{figure}

\section*{Problem 8}

We ommit the end points in this problem, since we have defined $v_i=u_i$ at these points.

\subsection*{a)}

For the same $n_\text{steps}$ used in \Fig{p7_num_sol} we find the absolute error $\Delta_i = \abs{u_i-v_i}$, where $u_i=u(x_i)$ from \Eq{p1_analytical_sol}. We plot $\log_{10}(\Delta_i(x_i))$ for different $n_\text{steps}$ in \Fig{p8_abs_err}. We see that the error reduces by a factor $\sim 10^{-2}$ as we increase the steps size by a factor $10$. This is consistent with the error we get from omitting the $\mathcal{O}(h^2)$ terms, since $h^2 \propto n_\mathrm{steps}^{-2}$. As seen from figure \ref{fig:p1_analytical_sol}, the analytical solution has its largest gradient at low $x$ values. Near the end-points, we get a small absolute error, since these points are approximated from the exact values at $x=0$ and $x=1$. With a lower gradient of $u(x)$ towards $x=1$, we see that the error desclines as $x$ increases.  


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{absolute_error.pdf}
    \caption{The logarithm of the absolute error as function of $x$ in the solution computed using \Algo{p6_general_algorithm} for different $n_\text{steps}$.}\label{fig:p8_abs_err}
\end{figure}

\subsection*{b)}

We now compute the relative error $\epsilon_i= \frac{\Delta_i}{\abs{u_i}}$, and plot $\log_{10}(\epsilon_i)$, shown in \Fig{p8_rel_err}. Since we're now scaling the error, the relative error is independent of $x$, and is determined solely by the value of $n_{\mathrm{steps}}$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{relative_error.pdf}
    \caption{The logarithm of the relative error as function of $x$ in the solution computed using \Algo{p6_general_algorithm} for different $n_\text{steps}$.}\label{fig:p8_rel_err}
\end{figure}

\subsection*{c)}
We compute solutions for $n_\text{steps}\in[10^1, 10^2, \dots, 10^7]$ and find the related maximum values, $\max{(\epsilon_i)}$, for each value of $n_\mathrm{steps}$. The results are presented in table \ref{tab:max_rel_error} with a corresponding plot shown in \Fig{p8_max_rel_err}. In the base-10 logarithm space, we see a linear decrease in relative error as the number of steps reaches $10^5$. For $n_\text{steps}\geq 10^6$ there is an increase in error, as we're dealing with numbers close to machine precision.

\begin{table}[h!]
    \caption{Maximum relative error per $n_{\mathrm{step}}$}
    \label{tab:max_rel_error}  
    \input{max_rel_error_table.tex}
\end{table}
% More discussion?

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{max_relative_error.pdf}
    \caption{The maximum relative error plotted over the number of steps used in \Algo{p6_general_algorithm}.}\label{fig:p8_max_rel_err}
\end{figure}

\section*{Problem 9}

\subsection*{a)}

We specialize the Thomas algorithm to the special case of the tridiagonal, symmetric Toeplitz matrix $A$ with signature $(-1,2,-1)$. This special algorithm is given in Algorithm \ref{algo:p9_special_algorithm} 

\begin{algorithm}[H]
    \caption{Special algorithm}\label{algo:p9_special_algorithm}
    \begin{algorithmic}
        \State $\tilde{b}_0 = b_0$ 
        \State $\tilde{g}_0 = g_0$
        \For{$i = 1, 2, ..., m-1$}
        \State $\tilde{b}_i = (i+2)/(i+1)$ 
        \State $\tilde{g}_i = g_i + \tilde{g}_{i-1} / \tilde{b}_{i-1}$
        \EndFor
        \State $v_{m-1} = \tilde{g}_{m-1} / \tilde{b}_{m-1}$
        \For{$i = m-2, m-3, ..., 0$}
        \State $v_i = (\tilde{g}_i + v_{i+1} ) / \tilde{b}_i$ 
        \EndFor
    \end{algorithmic}
\end{algorithm}

Here we have simply substituted for $a_i, b_i$ and $c_i$ in \Algo{p6_general_algorithm} and recognized that $(a_i/\tilde{b}_{i-1}) \cdot c_{i-1}=1/2$ for the first iteration, which yields the formula $\tilde{b}_i=(i+2)/(i+1)$. Similar considerations have been made to simplify the expressions for $\tilde{g}_i$ and $v_i$. 


\subsection*{b)}

\Algo{p9_special_algorithm} performs $1$ and $2$ FLOPs for computing $\tilde{b}_i$ and $\tilde{g}_i$, respectively, yielding $3(m-2)$ FLOPs in the first loop. $1$ FLOP is performed before the second loop, and $2 \cdot (m-2)$ FLOPs are performed in the second loop, yielding a total of $5(m-2)+1$ FLOPs in the special Thomas algorithm.

\subsection*{c)}

We implement \Algo{p9_special_algorithm} in our \CC\, script. 

\section*{Problem 10}

We write a code that for $n_\text{steps}\in[10^{1}, 10^{2}, \dots, 10^{6}]$ computes \Algo{p6_general_algorithm} and \Algo{p9_special_algorithm} 500 times, and finds the average duration of the two algorithms for each $n_\mathrm{steps}$. In addition, we find the root mean square error of the measurements. In \Fig{p10_timed} we visualise the results that are listed in table \ref{tab:thomas_algo}.

\begin{table}
    \caption{The mean $\mu$ and standard deviation $\sigma$ of time spent to execute the general and special Thomas algorithms}
    \label{tab:thomas_algo}  
    \include{thomas_timed_table.tex}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{algorithms_timed.pdf}
    \caption{ The plot shows timing results for running the general (blue) and special (green) Thomas algorithms. The average duration (dots), and corresponding RMS error in measurements (bars), of one run are plotted for different choices in number of steps.}\label{fig:p10_timed}
\end{figure}

In figure \ref{fig:p10_timed} we see that the special algorithm is quicker than the general one. The only noticeable feature is the standard deviation of the general algorithm for $n_\mathrm{steps}=10$. However, the standard deviations at small step sizes are on the order of micro seconds, where our result is relatively sensitive to hardware specifics. To see how the time differences compare to the number of FLOPs in the two algorithms, we tabulate $\mu_S/\mu_G$ for each step, together with the ratio between the number of FLOPs from the general and special algorithm, which are $8(m-2)+1$ and $5(m-2)+1$, respectively. The result is given in table \ref{tab:rel_time_diff}. The general algorithm performs slightly better than what it would if FLOPs were the only governing factor. This is expected, since memory handling and various other processes affect the timing. However, there appear to be a reasonable correspondence between difference in FLOPs and difference in time for the two algorithms.    

\begin{table}
    \include{rel_time_thomas_table.tex}
    \caption{The ratio of average times and ratio of FLOPs for the two algorithms for different number of steps.}
    \label{tab:rel_time_diff}
\end{table}

\appendix

\section{Derivation of the Thomas algorithm}\label{Apx:Thomas_algorithm}

In order to derive the Thomas algorithm, we monitor how a series of row operations can achieve the following relation:
\begin{align*}
    \begin{pmatrix}
        A & \vec{g} 
    \end{pmatrix}
    \sim 
    \begin{pmatrix}
        \mathbb{I}^n & \vec{v}
    \end{pmatrix}
\end{align*}
which is the same as:
\begin{align*}
    \begin{pmatrix}
        b_1 & c_1 & &  & g_1 \\
        a_2 & \ddots & \ddots & & g_2 \\ 
         & \ddots & & c_{n-1} & \vdots \\
          & & a_n & b_n & g_n
    \end{pmatrix}
    \sim 
    \begin{pmatrix}
        \mathbb{I}^n & \vec{v}
    \end{pmatrix}
\end{align*}
We have obtained $\mathbb{I}^n$ if all elements in $\vec{a}$ are equal to 1, and all elements in $\vec{b}$ and $\vec{c}$ are equal to 0. We firs remove the $\vec{a}$ vector by forward substitution. The first row will remain unchanged, but we need to perform row operations on the remaining rows:

\begin{align*}
    \tilde{R}_1 &= R_1 \\
    \tilde{R}_2 &= R_2 - \frac{a_2}{\tilde{b}_1}\tilde{R}_1\\
    \vdots & \\
    \tilde{R}_n &= R_n - \frac{a_n}{\tilde{b}_{n-1}}\tilde{R}_{n-1}
\end{align*}
After this forward substitution we have that all elements of $\vec{a}$ is 0, and:
\begin{align*}
    \tilde{b}_1 &= b_1 \\
    \tilde{b}_2 &= b_2 - \frac{a_2}{\tilde{b}_1} c_1 \\
    \vdots & \\
    \tilde{b}_n &= b_n - \frac{a_n}{\tilde{b}_{n-1}} c_{n-1}
\end{align*}
For $\vec{g}$ we have likewise:
\begin{align*}
    \tilde{g}_1 &= g_1 \\
    \tilde{g}_2 &= g_2 -  \frac{a_2}{\tilde{b}_1}\tilde{g}_1 \\
    \vdots &= \\
    \tilde{g}_n &= g_n - \frac{a_n}{\tilde{b}_{n-1}}\tilde{g}_{n-1}
\end{align*}

The next step is to get tid of $\vec{ c}$ and normalise $\vec{\tilde{b}}$ in order to obtain the identity matrix. This is done through backward substitution:
\begin{align*} 
    \tilde{R}^*_n &= \frac{\tilde{R}_n}{\tilde{b}_n} \\
    \tilde{R}^*_{n-1} &= \frac{\tilde{R}_{n-1} -  c_{n-1}\tilde{R}^*_n}{\tilde{b}_{n-1}} \\
    \vdots & \\
    \tilde{R}^*_1 &= \frac{\tilde{R}_{1} -  c_{1}\tilde{R}^*_2}{\tilde{b}_{1}}
\end{align*}
We then write in terms of $v_i = \tilde{g}^*_i$:
\begin{align*}
    v_n &= \tilde{g}^*_n = \frac{\tilde{g}_n}{\tilde{b}_n} \\
    v_{n-1} &= \frac{\tilde{g}_{n-1}- v_n c_{n-1}}{\tilde{b}_{n-1}}\\
    v_1 &= \frac{\tilde{g}_1 - v_2 c_1}{\tilde{b}_1}
\end{align*}


To summarize: We define $\tilde{b}_1 = b_1$ and $\tilde{g}_1 = g_1$. Through iteration, the follwing terms, valid for $i\in[2,\,n]$ become 

\begin{align*}
    \tilde{b}_i &= b_i - \frac{a_i}{\tilde{b}_{i-1}} c_{i-1} \\
    \tilde{g}_i &= g_i - \frac{a_i}{\tilde{b}_{i-1}}\tilde{g}_{i-1} 
\end{align*}
We now obtain an expression for $v_n=\tilde{g}^*_n = \tilde{g}_n/\tilde{b}_n$, and get the remaining elements by backwards iteration  
\begin{align*}
    v_{i} &= \frac{\tilde{g}_{i}- v_{i+1} c_{i}}{\tilde{b}_{i}} 
\end{align*}
for $i\in[n-1,\,1]$. 

\end{document}

