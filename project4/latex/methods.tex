\section{Theoretical background}\label{sec:theory}

In this section we present a brief introduction to the 2D Ising model, as well some theoretical background of phase transition phenomena and statistical mechanics. Additionally, we briefly describe the overall ideas behind the workings of Markov chain Monte Carlo (MCMC) methods, in particular how this is implemented with the Metropolis algorithm. Details regarding our own implementation of an MCMC algorithm will be presented in \Vetle{Section 69}.    

\subsection{2D Ising model}\label{subsec_theory:ising_2d}
The Ising model is a mathematical model used to model statistical properties of materials, such as ferromagnetism. It consists of discrete magnetic moments localized in a particular lattice. For simplicity, we will refer to the magnetic moments as "spins" throughout this report. Additionally, we will restrict our analysis to a square lattice in two dimensions without an external magnetic field. Each spin has a fixed position in the lattice, where they are allowed to interact with their immediate neighbors. We will denote a single spin as $s_i$, where the subscript $i$ refers to a certain position in the lattice. An individual spin can be in one of two possible states having a value of $s_i=+1$ or $s_i=-1$. We will refer to the \Vetle{(an arbitrary)} spin state of the entire lattice as a \textit{microstate}, which we will denote as $\nu$ \Vetle{(Don't want vector or hat/tilde, and other Greek letters may cause confusion)}. The lattice has dimension $(L\cross L)$, where we refer to $L$ as the lattice size. The number of \Vetle{(individual)} spins in the lattice is thus $N=L^2$. \Vetle{Explain that $\nu$ is a matrix?} The total energy of the system is given by 
\begin{equation} \label{eq:energy}
    E(\nu) = -J \sum_{\langle kl \rangle}^N s_k s_l,
\end{equation}
where $\langle kl\rangle$ indicates that the sum is taken over the nearest neighboring pairs of spins. \Vetle{Explain which neighbors are summed over.} The parameter $J$ represents the strength of interactions between neighboring spins, and we assume a constant $J>0$, so that energy is minimized when neighboring spins are aligned \Vetle{(Energy minimization sentence should perhaps be removed...)}. 

The total magnetization of the system will be given by the sum over all spins 
\begin{equation}\label{eq:total_magnetization}
    M(\nu) = \sum_{i}^N s_i.
\end{equation}

\subsection{Phase transitions and critical phenomena}\label{subsec_theory:PT_critical_phenomena}
\alert{This section is yet to be completed. I will write it more properly once I know we shall use it in practice.}

One of the most peculiar features of continuous phase transitions is that thermodynamic quantities are found to exhibit similar behavior for a variety of different systems. These phase transitions can be characterized by \textit{critical exponents}, where physical quantities behave according to power laws near the critical point. \Vetle{Mention universality classes?} For a 2D Ising model of infinite size, the behavior of the \Vetle{fill in...}

\begin{align}
    \expval{\abs{M}} & \propto \abs{T - T_c(L=\infty)}^\beta, \label{eq:crit_expo_mag} \\
    C_V & \propto \abs{T - T_c(L=\infty)}^{-\alpha}, \label{eq:crit_expo_heatcap} \\ 
    \chi & \propto \abs{T - T_c(L=\infty)}^{-\gamma}, \label{eq:crit_expo_mag_susc} \\ 
    \xi & \propto \abs{T - T_c(L=\infty)}^{-\nu}, \label{eq:crit_expo_corr_length}
\end{align}
where the critical exponents are $\beta=1/8$, $\alpha=0$, $\gamma=7/4$ and $\nu=1$. For our finite system, the largest possible correlation length is $\xi=L$, associated with $T=T_c(L)$, which is the critical temperature of our finite system. Using the constraint on $\xi$ yields the following relation 
\begin{equation}
    T_c(L) - T_c(L=\infty) = aL^{-1}, \label{eq:finite_size_scaling_relation}
\end{equation}  
where $a$ is a proportionality constant. \Vetle{I will connect the dots when I know how we will do this.}

The analytical value of $T_c(L=\infty)$ for the 2D Ising model with no external magnetic field has found to be \cite{Onsager_Ising2D}
\begin{equation}
    T_c(L=\infty) = \frac{2}{\ln(1+\sqrt{2})} J/k_B \approx 2.269\,J/k_B. \label{eq:onsager_critical_temperature}
\end{equation}


\subsection{Statistical mechanics}\label{subsec_theory:statistical_mechanics}

The probability of the system being in a microstate $\nu$ at a given energy and a constant temperature, $T$, is governed by the Boltzmann distribution
\begin{equation}\label{eq:boltzmann_distr}
    p_\nu (T) = \frac{1}{Z}e^{-\beta E(\nu)},
\end{equation} 
where $\beta=1/k_B T$ with $k_B$ being the Boltzmann constant. $Z$ is the partition function, defined as 
\begin{equation}\label{eq:partition_function}
    Z = \sum_\nu e^{-\beta E(\nu)},
\end{equation}  
where the sum goes over all possible microstates $\nu$. 

For an observable $Q(\nu)$, its expectation value is given by 
\begin{equation}\label{eq:observable_exp_val}
    \expval{Q} = \sum_{\nu} Q(\nu)\, p_\nu(T) = \frac{1}{Z} \sum_\nu Q(\nu) e^{-\beta E(\nu)}.
\end{equation}
\Vetle{Should perhaps include a paragraph of estimated averages, and how these converge to the true thermal averages (ergodic hypothesis (not the same ``ergodic'' that is mentioned in the MCMC subsection))} 



\subsection{Monte Carlo methods} \label{subsec_theory:MC_methods}

To study thermodynamic properties of the Ising model, we need to compute the partition function in Eq. \eqref{eq:partition_function}. However, this would require a sum over all possible microstates. For a lattice of size $L$ the total number of \Vetle{unique} microstates are $2^{L^2}=2^N$. This becomes an impossible task for the lattice sizes we are going to consider. To overcome this problem we may draw random samples of the system and use these samples to estimate thermal averages. A common choice for sampling the system is the Markov chain Monte Carlo (MCMC) method. A Markov chain is a stochastic process in which the outcome of an event is independent of the process's history. Loosely speaking, it can be regarded as a random walk in (\Vetle{our x-dimensional}) state space. There are several algorithms for constructing these Markov chains, but the one we will consider is the \textit{Metropolis-Hastings} algorithm. 

\subsection{Metropolis algorithm} \label{subsec_theory:metropolis_algorithm}
The Metropolis algorithm uses a Markov process to generate multiple samples of microstates that approximate a Boltzmann weighted ensemble. A Markov process is defined by a transition probability, $W(\nu \to \nu')$, which is the probability of the system to transition from a state $\nu$ to any given state $\nu'$. For the Markov chain to reach the desired distribution we have to fulfill two conditions.  The first condition is \textit{ergodicity} (\Vetle{I will add a footnote here later}), which states that the system has a non-zero probability of going from any state to any other state with a finite sequence of transitions. The second condition is \textit{detailed balance}, meaning that we require each transition to be reversible. That is, the probability for the system to be in a state $\nu$ and transition to a state $\nu'$ is equal to the probability for the system to be in state $\nu'$ and transition to state $\nu$. Mathematically, this can be formulated as 
\begin{equation}
    P(\nu)W(\nu\to \nu') = P(\nu') W(\nu'\to \nu), \label{eq:detailed_balance}
\end{equation} 
where $P(\nu)$ is the distribution of the Markov process \Vetle{(Improve explanation)}. It can be shown that when the conditions of ergodicity and detailed balance are fulfilled, the distribution $P(\nu)$ is both unique and stationary. \Vetle{I'm not sure if this is mathematically rigorous. Citation should definitely be considered (I found this on Wikipedia).}

A naive approach for sampling configurations would be to sample random microstates uniformly. This has the disadvantage of including microstates which the system is unlikely to be in, and many computations would be required to get a satisfying distribution. Instead, the Metropolis algorithm works by choosing the Boltzmann distribution to sample spins, i.e. $P(\nu)=p_\nu(T)$. If we insert this into Eq. \eqref{eq:detailed_balance} and rewrite it, we get 

\alert{I should not introduce delta E here. I will change later}

\begin{equation}\label{eq:detailed_balance_ratio}
    \frac{W(\nu\to \nu')}{W(\nu'\to \nu)} = \frac{p_{\nu'}(T)}{p_\nu(T)} = e^{-\beta \Delta E},
\end{equation}
where we defined $\Delta E \equiv E(\nu')-E(\nu)$. Importantly, in Eq. \eqref{eq:detailed_balance_ratio} we now see that the partition function has vanished. In general, $W(\nu\to\nu')$ is unknown, and the Metropolis algorithm works by assuming that it can be written as the product of the two probabilities $A(\nu\to\nu')$ and $T(\nu\to\nu')$, the probability of accepting the transition from $\nu\to\nu'$ and the probability of making the transition to $\nu'$ being in state $\nu$. One common choice is to assume a symmetric transition probability $T(\nu\to\nu')=T(\nu'\to\nu)$. Eq. \eqref{eq:detailed_balance_ratio} then reads 
\begin{equation}\label{eq:acceptance_rate}
    \frac{A(\nu\to\nu')}{A(\nu'\to\nu)}=e^{-\beta \Delta E}. 
\end{equation}
Since energy tends to be minimized, a natural choice is to always accept new states if it has a lower energy than the initial state since $\Delta E<0$ corresponds to transitioning to a state with a higher probability. Assuming an acceptance probability of $1$ for a transition resulting in a lower energy, we can express the transition probability $A(\nu\to\nu')$ as 
\begin{equation} \label{eq:metropolis_acceptance}
    A(\nu\to\nu') = \min\closed{1,e^{-\beta \Delta E}}.
\end{equation}  
With Eq. \eqref{eq:metropolis_acceptance}, we will always transition to states with lower energies. Since ergodicity must be fulfilled, we may not reject any transition that doesn't lower the energy. In \Vetle{Section 420} we discuss how we implement this in practice.    


\section{Implementations}\label{sec:implementations}
\alert{Have we mentioned PBC yet?}


\subsection{Energy change due to single spin flip}\label{subsec_implementations:de_from_single_flip}
When flipping a single spin there is a limited number of possible values $\Delta E$ can take. Consider an arbitrary spin, $s_k$, in a lattice with $L>2$. The energy contribution from this spin's interaction with its neighbors is 
\begin{equation}\label{eq:single_spin_energy}
    E(s_k) = -J s_k \sum_{\langle l \rangle} s_l.
\end{equation}   
Flipping this spin corresponds to $s_k\to -s_k$. The change in energy from this is $\Delta E=E(-s_k)-E(s_k)$, resulting in the following expression:
\begin{align}
        \Delta E &= -J(-s_k) \sum_{\langle l \rangle} s_l - (-J s_k) \sum_{\langle l \rangle} s_l \nonumber \\ 
        &= 2Js_k \sum_{\langle l \rangle} s_l. \label{eq:DeltaE_from_single_spin_flip}
\end{align}
Since each of the spins $s_l$ takes a value of $-1$ or $+1$, taking the sum over four spins can only yield a value of $\sum_{\langle l \rangle}s_l=\{-4,-2,0,2,4\}$. With $s_k=\pm1$, it's evident that flipping a single spin results in five possible values of $\Delta E$, which are 
\begin{equation}\label{eq:DeltaE_possible_values}
    \Delta E = \{-8J,-4J,0,4J,8J\}. 
\end{equation}  

\Vetle{Relate this to metropolis.}

Flipping a spin also yields a change in magnetization, $\Delta M\equiv M(\nu')-M(\nu)$, from the transition $s_k\to s_k'=-s_k$. The new magnetization is easily seen to be 
\begin{equation} \label{eq:magnetization_from_single_flip}
    M(\nu') = M(\nu) + 2 s_k',
\end{equation} 
with $\Delta M={-2,+2}$ as the only possibilities, corresponding to initial spin values of $s_k={+1,-1}$, respectively.   

\subsection{Metropolis algorithm}\label{subsec_implementations:metropolis}
To implement the Metropolis algorithm we determine whether to accept a spin flip by comparing $A(\nu\to\nu')$ from Eq. \eqref{eq:metropolis_acceptance} with a random number $r\sim \mathcal{U}(0,1)$ \Vetle{(should it be r in U(0,1)?)}. This method guarantees that the proposed spin flip is accepted if $\Delta E<0$. Additionally, transitions yielding $\Delta E>0$ have a higher probability of being accepted if the associated reduction of the Boltzmann distribution is small. 

\alert{w in the enumeration below must be defined/introduced.}

To sample spins, we will draw spins in the lattice according to a uniform random distribution. The basic outline of our MCMC algorithm is as follows:
\begin{enumerate}
    \item Pick a random spin in the lattice, $s_k$.
    \item Compute $w=\exp(-\beta \Delta E)$ resulting from spin flip  
    \item If $w \geq r$: Accept flip, update $E$ and $M$. 
    \item If $w<r$: Reject proposed flip.
    \item Repeat.   
\end{enumerate} 
Repeating this process $N=L^2$ number of times constitutes one MC \textit{cycle}. After each cycle, we use the final values $E$ and $M$ to store as our samples. 

and we accept the flip, and proceed by picking a new spin at random.   

\Vetle{The algorithm will be changed later.}
\begin{algorithm}[H]
    \caption{Flip}\label{algo:flip}
    \begin{algorithmic}
        \State Pick $s_k$ on a random position in the lattice.
        \State Compute $\Delta E$
        \If{$\Delta E\leq0$}
            \State $s_k = s_k * (-1)$
            \State $E = E + \Delta E$
            \State $M = M + 2 * s_k$
        \Else
            \State $w=\exp(-\beta \Delta E)$
            \State $r\in\mathcal{U}(0,1)$
            \If{$r\leq w$}
                \State $s_k = s_k * (-1)$
                \State $E = E + \Delta E$
                \State $M = M + 2 * s_k$
            \EndIf
        \EndIf
    \end{algorithmic}
\end{algorithm}



\section{Methods}\label{sec:methods}

\subsection{Analytical comparison}\label{subsec_methods:analytical_test}
To test our implementation, we will first consider a lattice of size $L=2$ for which we can compute the analytical solution. The quantities we will consider are $\avge,\avgm,C_V$ and $\chi$. The analytical derivation of these quantities for $L=2$ is given in Appendix \ref{app:analytical_expressions}. These analytical quantities will be compared to the numerical estimates after different number of cycles have been run. \Vetle{Bleh, drittforklaring.}

\subsection{Equilibriation time}\label{subsec_methods:equilibriation_time}
When initializing the system is unlikely to be in a state near equilibrium. This means that a majority of the cycles we run in the beginning will consist of transitions such that the system approaches an equilibrium. After a certain number of iterations the samples we draw are likely to oscillate around the equilibrium values. The number of cycles needed to reach this equilibrium is called the \textit{equilibriation time}, $t_\mathrm{eq}$. To reduce the number of cycles needed to obtain accurate estimates of thermal averages, we only include samples drawn after we have performed $t_\mathrm{eq}$ equilibriation steps. 

To estimate $t_\mathrm{eq}$, we will consider a lattice with $L=20$, and plot $\avge$ and $\avgm$ as a function of cycles. From this we will choose a rough estimate of $t_\mathrm{eq}$. We will do this for $T=1$\Vetle{unit} and $T=2.4$, and in both cases study the result from a lattice that is initially ordered (all spins up) and a lattice that is unordered (random). \Vetle{Describe ordered/disordered}. The equilibriation time is chosen as the approximate number of cycles performed until we see that the measured quantities oscillate around a somewhat stable value. This number of equilibriation steps will be used in the remaining methods, and unless stated otherwise, all the following methods concern drawing samples after we have equilibriated the system.     

\Vetle{Should reference the analytical equilibriation result, and why we don't opt for it.}


\subsection{Estimating the probability distribution}\label{subsec_methods:histogram}
After the equilibriation time has been estimated we wish to estimate the probability distribution of the energy $p_\eps(T)$. To do this, we sample the $\eps$ from a total of $N=10^5$ cycles, and plot histogram of the sampled energies. We will consider a lattice of size $L=20$. From Eq. \eqref{eq:DeltaE_possible_values}, we know that the smallest change in energy per spin for a lattice with $N=400$ spins is $\min(\Delta\eps)=0.01\,\mathrm{J}$, which is the batch size we will consider for the histogram. We plot the normalized distribution for $T=1$ and $T=2.4$, and for both temperatures we compute and estimate of the mean and the variance of the two distributions.     


\subsection{Phase transitions}\label{subsec_methods:PT}