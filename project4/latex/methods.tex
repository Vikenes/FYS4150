\section{Theoretical background}\label{sec:theory}

In this section we present a brief introduction to the 2D Ising model, as well some theoretical background of phase transition phenomena and statistical mechanics. Additionally, we briefly describe the overall ideas behind the workings of Markov chain Monte Carlo (MCMC) methods, in particular how this is implemented with the Metropolis algorithm. A more detailed explanation of our implementations will be presented in Sec. \ref{sec:implementations}.    

\subsection{The 2D Ising model}\label{subsec_theory:ising_2d}
The Ising model is a mathematical model used to model statistical properties of materials, such as ferromagnetism. It consists of discrete magnetic moments localized in a particular lattice. For simplicity, we will refer to the magnetic moments as "spins" throughout this report. Additionally, we will consider a square lattice of dimension $(L\cross L)$, where we refer to $L$ as the lattice size. Each spin has a fixed position in the lattice, where they are allowed to interact with their immediate neighbors. We will denote a single spin as $s_i$, where the subscript $i$ refers to a certain position in the lattice. An individual spin can be in one of two possible states having a value of $s_i=+1$ or $s_i=-1$. We will refer to an arbitrary spin state of the entire lattice as a \textit{microstate}, which we will denote as $\nu$\footnote{In practice, $\nu$ can thus be viewed as an $L\cross L$ matrix.}. The number of individual spins in the lattice is thus $N=L^2$. We will limit our analysis to the case of no external magnetic field. The total energy of the system is given by 
\begin{equation} \label{eq:energy}
    E(\nu) = -J \sum_{\langle kl \rangle}^N s_k s_l,
\end{equation}
where $\langle kl\rangle$ indicates that the sum is taken over the nearest neighboring pairs of spins. We will in this report impose periodic boundary conditions, so all spins have exactly four neighbors (left, right, up and down). The parameter $J$ represents the strength of interactions between neighboring spins, and we assume a constant $J>0$, so that the energy is lower when neighboring spins are aligned. 

The total magnetization of the system will be given by the sum over all spins 
\begin{equation}\label{eq:total_magnetization}
    M(\nu) = \sum_{i}^N s_i.
\end{equation}

\subsection{Statistical mechanics}\label{subsec_theory:statistical_mechanics}
The probability of the system being in a microstate $\nu$ at a given energy and a constant temperature, $T$, is governed by the Boltzmann distribution
\begin{equation}\label{eq:boltzmann_distr}
    p_\nu (T) = \frac{1}{Z}e^{-\beta E(\nu)},
\end{equation} 
where $\beta=1/k_B T$ with $k_B$ being the Boltzmann constant. $Z$ is the partition function, defined as 
\begin{equation}\label{eq:partition_function}
    Z = \sum_\nu e^{-\beta E(\nu)},
\end{equation}  
where the sum goes over all possible microstates $\nu$. 

For an observable $Q(\nu)$, its expectation value is given by 
\begin{equation}\label{eq:observable_exp_val}
    \expval{Q} = \sum_{\nu} Q(\nu)\, p_\nu(T) = \frac{1}{Z} \sum_\nu Q(\nu) e^{-\beta E(\nu)}.
\end{equation}
\Vetle{Should perhaps include a paragraph of estimated averages, and how these converge to the true thermal averages (ergodic hypothesis (not the same ``ergodic'' that is mentioned in the MCMC subsection))} 

Two quantities we will focus on in this report are the specific heat capacity, $C_V$, and the magnetic susceptibility, $\chi$, both normalized to the number of spins, $N$, which are defined as 
\begin{align}
    C_V &= \frac{1}{N}\frac{1}{k_B T^2} \closed{\expval{E^2} - \expval{E}^2}, \label{eq:C_V} \\ 
    \chi &= \frac{1}{N} \frac{1}{k_B T} \closed{\expval{M^2} - \expval{\abs{M}}^2}, \label{eq:chi}
\end{align} 
where the various average quantities are obtained from Eq. \eqref{eq:observable_exp_val}. 

\subsection{Phase transitions and critical phenomena}\label{subsec_theory:PT_critical_phenomena}
One of the most peculiar features of continuous phase transitions is that thermodynamic quantities are found to exhibit similar behavior for a variety of different systems. These phase transitions can be characterized by \textit{critical exponents}, where physical quantities behave according to power laws near the critical point, i.e. a critical temperature. Near the critical temperature, $\expval{\abs{M}},\,C_V$ and $\chi$ behave according to the following power laws: 
\begin{align}
    \expval{\abs{M}} & \propto \abs{T - T_c(L=\infty)}^\beta, \label{eq:crit_expo_mag} \\
    C_V & \propto \abs{T - T_c(L=\infty)}^{-\alpha}, \label{eq:crit_expo_heatcap} \\ 
    \chi & \propto \abs{T - T_c(L=\infty)}^{-\gamma}, \label{eq:crit_expo_mag_susc}
\end{align}
where the critical exponents of the 2D Ising model we consider are $\beta=1/8$\footnote{The exponent $\beta$ is not be confused with the inverse temperature.}, $\alpha=0$ and $\gamma=7/4$ \cite{crit_expo}. Both $C_V$ and $\chi$ will therefore diverge near the critical temperature\footnote{A critical exponent of $\alpha=0$ means that $C_V$ diverges logarithmically.}. $T_c(L=\infty)$ is the critical temperature according to a square lattice Ising model of infinite size. Additionally, the correlation between spins is characterized by the correlation length, $\xi$, which has the following power law behavior near the critical temperature 
\begin{equation}
    \xi \propto \abs{T - T_c(L=\infty)}^{-\nu}, \label{eq:crit_expo_corr_length}
\end{equation}
where $\nu=1$\footnote{Not to be confused with the microstate, $\nu$.}. Our analysis is limited to systems of finite extent, and it can be shown \Vetle{(ref?)} that the correlation length is proportional to the lattice size near the critical temperature. For $T=T_c(L)$, we then have $\xi\propto L$. Inserting this into Eq. \eqref{eq:crit_expo_corr_length}, we get a relation between the lattice size $L$ and the associated critical temperature, $T_c(L)$
\begin{equation}
    T_c(L) - T_c(L=\infty) = aL^{-1}, \label{eq:finite_size_scaling_relation}
\end{equation}  
where $a$ is a proportionality constant. By estimating the critical temperature at different lattice sizes, we can perform linear regression to obtain an estimate of $T_c(L=\infty)$. This estimate can be compared to the analytical value of $T_c(L=\infty)$, which for the 2D Ising model with no external magnetic field is \cite{Onsager_Ising2D}
\begin{equation}
    T_c(L=\infty) = \frac{2}{\ln(1+\sqrt{2})} J/k_B \approx 2.269\,J/k_B. \label{eq:onsager_critical_temperature}
\end{equation}


\subsection{Monte Carlo methods} \label{subsec_theory:MC_methods}

To study thermodynamic properties of the Ising model, we need to compute the partition function in Eq. \eqref{eq:partition_function}. However, this would require a sum over all possible microstates. For a lattice of size $L$ the total number of unique microstates are $2^{L^2}=2^N$. This becomes an impossible task for the lattice sizes we are going to consider. To overcome this problem we may draw random samples of the system and use these samples to estimate thermal averages. A common choice for sampling the system is the Markov chain Monte Carlo (MCMC) method. A Markov chain is a stochastic process in which the outcome of an event is independent of the process's history. Loosely speaking, it can be regarded as a random walk in (\Vetle{our x-dimensional}) state space. There are several algorithms for constructing these Markov chains, but the one we will consider is the \textit{Metropolis-Hastings} algorithm.

\subsection{Metropolis algorithm} \label{subsec_theory:metropolis_algorithm}
The Metropolis algorithm uses a Markov process to generate multiple samples of microstates that approximate a Boltzmann weighted ensemble. A Markov process is defined by a transition probability, $W(\nu \to \nu')$, which is the probability of the system to transition from a state $\nu$ to any given state $\nu'$. For the Markov chain to reach the desired distribution we have to fulfill two conditions.  The first condition is \textit{ergodicity} (\Vetle{I will add a footnote here later}), which states that the system has a non-zero probability of going from any state to any other state with a finite sequence of transitions. The second condition is \textit{detailed balance}, meaning that we require each transition to be reversible. That is, the probability for the system to be in a state $\nu$ and transition to a state $\nu'$ is equal to the probability for the system to be in state $\nu'$ and transition to state $\nu$. Mathematically, this can be formulated as 
\begin{equation}
    P(\nu)W(\nu\to \nu') = P(\nu') W(\nu'\to \nu), \label{eq:detailed_balance}
\end{equation} 
where $P(\nu)$ is the distribution of the Markov process \Vetle{(Improve explanation)}. It can be shown that when the conditions of ergodicity and detailed balance are fulfilled, the distribution $P(\nu)$ is both unique and stationary. \Vetle{I'm not sure if this is mathematically rigorous. Citation should definitely be considered (I found this on Wikipedia).}

A naive approach for sampling configurations would be to sample random microstates uniformly. This has the disadvantage of including microstates which the system is unlikely to be in, and many computations would be required to get a satisfying distribution. Instead, the Metropolis algorithm works by choosing the Boltzmann distribution to sample spins, i.e. $P(\nu)=p_\nu(T)$. If we insert this into Eq. \eqref{eq:detailed_balance} and rewrite it, we get 

\alert{I should not introduce delta E here. I will change later}

\begin{equation}\label{eq:detailed_balance_ratio}
    \frac{W(\nu\to \nu')}{W(\nu'\to \nu)} = \frac{p_{\nu'}(T)}{p_\nu(T)} = e^{-\beta \Delta E},
\end{equation}
where we defined $\Delta E \equiv E(\nu')-E(\nu)$. Importantly, in Eq. \eqref{eq:detailed_balance_ratio} we now see that the partition function has vanished. In general, $W(\nu\to\nu')$ is unknown, and the Metropolis algorithm works by assuming that it can be written as the product of the two probabilities $A(\nu\to\nu')$ and $T(\nu\to\nu')$, the probability of accepting the transition from $\nu\to\nu'$ and the probability of making the transition to $\nu'$ being in state $\nu$. One common choice is to assume a symmetric transition probability $T(\nu\to\nu')=T(\nu'\to\nu)$. Eq. \eqref{eq:detailed_balance_ratio} then reads 
\begin{equation}\label{eq:acceptance_rate}
    \frac{A(\nu\to\nu')}{A(\nu'\to\nu)}=e^{-\beta \Delta E}. 
\end{equation}
Since energy tends to be minimized, a natural choice is to always accept new states if it has a lower energy than the initial state since $\Delta E<0$ corresponds to transitioning to a state with a higher probability. Assuming an acceptance probability of $1$ for a transition resulting in a lower energy, we can express the transition probability $A(\nu\to\nu')$ as 
\begin{equation} \label{eq:metropolis_acceptance}
    A(\nu\to\nu') = \min\closed{1,e^{-\beta \Delta E}}.
\end{equation}  
With Eq. \eqref{eq:metropolis_acceptance}, we will always transition to states with lower energies. Since ergodicity must be fulfilled, we may not reject any transition that doesn't lower the energy. In Sec. \ref{subsec_implementations:metropolis} we discuss how we implement this in practice.    


\section{Implementations}\label{sec:implementations}

For our computations we will use $J$ and $J/k_B$ as the base units for energy and temperature, respectively. \Vetle{(Move this paragraph to theory maybe?)}. 

\subsection{Energy change due to single spin flip}\label{subsec_implementations:de_from_single_flip}
When flipping a single spin there is a limited number of possible values $\Delta E$ can take. Consider an arbitrary spin, $s_k$, in a lattice with $L>2$. The energy contribution from this spin's interaction with its neighbors is 
\begin{equation}\label{eq:single_spin_energy}
    E(s_k) = -J s_k \sum_{\langle l \rangle} s_l.
\end{equation}   
Flipping this spin corresponds to $s_k\to -s_k$. The change in energy from this is $\Delta E=E(-s_k)-E(s_k)$, resulting in the following expression:
\begin{align}
        \Delta E &= -J(-s_k) \sum_{\langle l \rangle} s_l - (-J s_k) \sum_{\langle l \rangle} s_l \nonumber \\ 
        &= 2Js_k \sum_{\langle l \rangle} s_l. \label{eq:DeltaE_from_single_spin_flip}
\end{align}
Since each of the spins $s_l$ takes a value of $-1$ or $+1$, taking the sum over four spins can only yield a value of $\sum_{\langle l \rangle}s_l=\{-4,-2,0,2,4\}$. With $s_k=\pm1$, it's evident that flipping a single spin results in five possible values of $\Delta E$, which are 
\begin{equation}\label{eq:DeltaE_possible_values}
    \Delta E = \{-8,-4,0,4,8\}\eunit. 
\end{equation}  
Flipping a single spin also yields a change in magnetization, $\Delta M\equiv M(\nu')-M(\nu)$, from the transition $s_k\to s_k'=-s_k$. The new magnetization due to this transition is easily seen to be $M(\nu') = M(\nu) + 2 s_k'$, where the possible values of $\Delta M$ are  
\begin{equation} \label{eq:DeltaM_possible_values}
    \Delta M=\{-2,+2\},
\end{equation} 
corresponding to initial spin values of $s_k=\{+1,-1\}$, respectively.   

Eqs. \eqref{eq:DeltaE_possible_values} and \eqref{eq:DeltaM_possible_values} allow for increased efficiency in our numerical implementation of the Metropolis algorithm, which we will discuss in the following section.  

\subsection{Metropolis algorithm}\label{subsec_implementations:metropolis}
To implement the Metropolis algorithm we determine whether to accept a spin flip by comparing $A(\nu\to\nu')$ from Eq. \eqref{eq:metropolis_acceptance} with a random number $r\sim \mathcal{U}(0,1)$ \Vetle{(should it be r in U(0,1)?)}. This method guarantees that the proposed spin flip is accepted if $\Delta E<0$. Additionally, transitions yielding $\Delta E>0$ have a higher probability of being accepted if the associated reduction of the Boltzmann distribution is small. 

To sample spins, we will draw spins in the lattice according to a uniform random distribution. Below, we present a simple outline of the Metropolis algorithm we use to sample spins. 
\begin{enumerate}
    \item Pick a random spin in the lattice, $s_k$.
    \item Compute resulting $\Delta E$ if spin is flipped, according to Eq. \eqref{eq:DeltaE_from_single_spin_flip}.  
    \item If $\exp(-\beta\Delta E) \geq r$: Accept flip, 
    \subitem $s_k\to-s_k$, 
    \subitem $E\to E+\Delta E$, 
    \subitem$M\to M+\Delta M$. 
    \item If $\exp(-\beta\Delta E)<r$: Reject flip.
    \item Repeat.   
\end{enumerate} 
Repeating this process $N=L^2$ number of times constitutes one MC \textit{cycle}. Using Eq. \eqref{eq:DeltaE_possible_values}, we compute the five possible values of $\exp(-\beta\Delta E)$ before we begin flipping the spins, rather than evaluating the exponential factor for each spin flip we consider. After one cycle is completed, we store the final energy and magnetization of our lattice, giving is one sample. The average energy and magnetization of the lattice are thus obtained by summing up the energies and magnetizations from each cycle, and divide by the number of cycles we have run, which we denote as $\nmc$. 

\subsection{Parallelization}\label{subsec_implementations:parallelization}
When we are going to investigate phase transitions, we will have to run MC simulations for multiple temperature values. In order to speed up the process, we will in that case resort to parallelization, where we use the \Vetle{Fancy text format} OpenMP parallelization method. \Vetle{Briefly describe the workings of OMP}. We have the freedom to implement parallelization at different levels in our simulations, but the one will consider is to parallelize the temperature iterations, meaning that individual threads are used to run MC simulations at one temperature simultaneously.    


\section{Methods}\label{sec:methods}
\alert{epsilon and little m is not defined yet. Don't know where to put them.}

\subsection{Analytical comparison}\label{subsec_methods:analytical_test}
To test our implementation, we will first consider a lattice of size $L=2$ for which we can compute the analytical solution. The quantities we will consider are $\avge,\avgm,C_V$ and $\chi$. The analytical derivation of these quantities for $L=2$ is given in Appendix \ref{app:analytical_expressions}. We will estimate the average values numerically for $\nmc=10^2,10^3,10^4,10^5$, and compare to the analytical results.

\subsection{Equilibration time}\label{subsec_methods:equilibration_time}
When initializing the system is unlikely to be in a state near equilibrium. This means that a majority of the cycles we run in the beginning will consist of transitions such that the system approaches an equilibrium. After a certain number of iterations the samples we draw are likely to oscillate around the equilibrium values. The number of cycles needed to reach this equilibrium is called the \textit{equilibration time}. For the sake of consistency however, we will denote it with $N_\mathrm{eq}$, to emphasize that we're referring to a number of MC cycles. To reduce the total number of cycles needed to obtain accurate estimates of thermal averages, we only include samples drawn after we have performed $N_\mathrm{eq}$ cycles initially. \Vetle{Mention time independent thermal averages?}. 

To determine a reasonable choice for $N_\mathrm{eq}$, we will consider a lattice with $L=20$, and plot $\avge$ and $\avgm$ as a function of cycles. We will do this for $T=1\tunit$ and $T=2.4\tunit$. For both temperatures, we study the result from a lattice with an ordered initialization (all spins pointing up) and a lattice with an unordered initialization (spins aligned with a uniform random distribution). Based on these plots, we estimate a reasonable choice for $\nequi$ by roughly considering the number of $\nmc$ required for the estimated averages to stabilize. The value of $\nequi$ we obtain from this analysis is the value we will choose for further analysis, unless stated otherwise. When we later refer to sampling the system, all discussions will concern drawing samples after we have equilibriated the system.     

\Vetle{Should reference the analytical equilibration result, and why we don't opt for it. In discussion?}


\subsection{Estimating the probability distribution}\label{subsec_methods:histogram}
Next, we wish to estimate the probability distribution of the energy, $p_\eps(T)$, for a lattice with $L=20$. To do this, we sample $\eps$ from a total of $\nmc=10^5$ subsequent cycles, for $T=1\tunit$ and $T=2.4\tunit$. From Eq. \eqref{eq:DeltaE_possible_values} we know that the smallest change in $\eps$ with $N=400$ spins is $\min(\Delta\eps)=0.01\,\mathrm{J}$. We estimate the probability distributions for the two energies by creating normalized histograms, using $\eps=0.01\eunit$ for the bin width. For both temperatures, we also estimate the mean and variance of the distributions.  

\alert{Where to put methods regarding parallelization time tests? Ahh, I know! The Appendix}

\subsection{Phase transitions}\label{subsec_methods:PT}
To estimate the critical temperature for the 2D Ising model of finite extent, $T_c(L)$, we will first study the temperature behavior of $C_V$ and $\chi$ for systems of different sizes. From Eq. \eqref{eq:onsager_critical_temperature} we know the analytical critical temperature in terms of an infinite sized lattice. To estimate the critical temperature of finite sized lattices, we begin by computing $C_V$ and $\chi$ for fifty equally spaced values of $T\in[2,\,2.5]\tunit$, corresponding to $\Delta T = 0.01\tunit$. We do this for lattices of size $L=40,60,80,100$, using $N_\mathrm{cycles}=10^5$ cycles at each temperature for all lattice sizes. According to Eqs. \eqref{eq:crit_expo_heatcap} and \eqref{eq:crit_expo_mag_susc}, we expect a diverging behavior of $C_V$ and $\chi$ near the critical temperatures. From our initial result, we get a rough estimate of the temperature range in which phase transition occurs, by seeing where $C_V$ and $\chi$ exhibit diverging behavior. We then proceed by computing $C_V$ and $\chi$ for $100$ temperatures in range near the critical temperature. We will then increase the number of cycles by a factor of $10$ in order to suppress large variations between subsequent temperature steps. 

\Vetle{Mention how we extract the point of divergence}

From our estimated values of $T_c(L)$, we can use Eq. \eqref{eq:finite_size_scaling_relation}, and plot the four values of $T_c(L)$ as a function of $L^{-1}$. From this, we can perform linear regression, and obtain an estimate of $T_c(L=\infty)$ from the intercept of our linear regression. 

